---
title: "Project DS502"
author: "Abhishek Shah, Mahdi Alouane, Rahul Pande and Sam Longenbach"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      tidy=TRUE,
                      fig.align='center',
                      tidy.opts=list(width.cutoff=60))
# https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
```
Import the training and testing datasets while converting white spaces to NAs as well
```{r , echo =TRUE}
train.data <- read.csv('./bigmart_train.csv', stringsAsFactors = FALSE, na.strings = c(""))
test.data <- read.csv('./bigmart_test.csv', stringsAsFactors = FALSE, na.strings = c(""))
```
Then we first check for the presence of NA values.
```{r , echo =TRUE}
check.na.values <- function(df){
  colSums(apply(df, 2, is.na))
}
check.na.values(train.data)
check.na.values(test.data)
```

We have missing values in `Item_Weight` and `Outlet_Size`.


Now, we check the frequencies of categorical variables.

```{r , echo =TRUE}
col_types = lapply(train.data, class)
char_cols = names(col_types[col_types == 'character'])
lapply(train.data[, setdiff(char_cols, c("Item_Identifier"))], table)
```


We aggregate on outlet level to impute outlet size
```{r , echo =TRUE}
setdiff(unique(train.data$Outlet_Identifier), unique(test.data$Outlet_Identifier))
setdiff(unique(test.data$Outlet_Identifier), unique(train.data$Outlet_Identifier))
```

We see that there are no new stores in the test data that are not already encountered in the training data.

```{r , echo =TRUE}
library(tidyverse)
item_weights <- train.data %>%
  group_by(Item_Identifier) %>%
  summarise(
    l_u_weights = length(unique(Item_Weight)),
    u_weights = paste(sort(unique(Item_Weight), na.last = TRUE), collapse = " | ")) %>%
  arrange(desc(u_weights))

head(item_weights, 10)
```

We see that in some places weights are NA whereas it is not NA in other rows for the same item. We can just use weights from other observations where weight is not NA (For the same item). For this purpose the whole train and test datasets have been used to impute the missing information. Hence, we define the following function for treating these missing values.

```{r}
fill_missing_values <- function(file) {
  train_file <- read.csv("./bigmart_train.csv")
  test_file <- read.csv("./bigmart_test.csv")
  all <- rbind(subset(train_file, select = -Item_Outlet_Sales), test_file)
  train_file_i <- which(is.na(file$Item_Weight))
  
  for (i in train_file_i) {
    id <- file[i,]$Item_Identifier
    weight <- all[ all$Item_Identifier == as.character(id) & !is.na(all$Item_Weight),]$Item_Weight[1]
    file[i,]$Item_Weight <- weight
  }
  
  return (file)
  
}
```

Then, we call it and check if the problem is resolved.

```{r , echo =TRUE}
train.data <- fill_missing_values(train.data)
test.data <- fill_missing_values(test.data)
check.na.values(train.data)
check.na.values(test.data)
```

Since, there is no more missing values for the feature `Item_Weight`, we treat in the following section the missing values for the feature `Outlet_Size`.

Given that `Outlet_Size` is an outlet specific attribute, we first begin by printing all the outlets available in our training set (10 outlets in total). Hence, we figure out that the 2410 missing values in training set belong to only 3 outlets and the size of these outlets is also missing in the testing set.

```{r}
unique_outs <- unique(train.data %>% select(starts_with("Outlet_")))
unique_outs[,c(1,3)]
```

In order to achieve this, we start by transforming the categorical attributes into dummy variables using `One Hot Encoding` as shown below:

``` {r}
library(DMwR)
library(fastDummies)

train_num <- dummy_cols(unique_outs, select_columns = c('Outlet_Establishment_Year','Outlet_Location_Type','Outlet_Size','Outlet_Type'))
train_num$Outlet_Size_NA <- NULL
na_indices <- which(is.na(train_num$Outlet_Size))
train_num[is.na(train_num$Outlet_Size),c('Outlet_Size_Small','Outlet_Size_Medium','Outlet_Size_High')] <- NA
names(train_num[,-c(1:5)])
```

Then, we predict the missing values for `Outlet_Size` using the K-Nearest Neighbors with K=5. The algorithm reaches out for the 5 closest neighbors (after scaling) for each observation where the attribute is missing and according to a vote assigns a score using a weighted average (`meth='weighAvg`). Therefore, we compute the maximum among the three possible values (Small, Medium and High) and assign it to the corresponding observation.

```{r}
train_num_imp <- knnImputation(train_num[,-c(1:5)], k = 5, scale=T)
for (i in na_indices) {
  out_sizes <- train_num_imp[i,c('Outlet_Size_Medium','Outlet_Size_High','Outlet_Size_Small')]
  max_out_size <- gsub('Outlet_Size_','',colnames(out_sizes)[apply(out_sizes,1,which.max)])
  train.data[train.data$Outlet_Identifier == train_num[i,]$Outlet_Identifier,]$Outlet_Size <- max_out_size
  unique_outs[unique_outs$Outlet_Identifier == train_num[i,]$Outlet_Identifier,]$Outlet_Size <- max_out_size
}
```

We can see here the missing values and their prediction according to 5NN.

```{r}
unique_outs[na_indices,]
train_num_imp[na_indices,c('Outlet_Size_Medium','Outlet_Size_High','Outlet_Size_Small'
                                )]
```

Finally, we check to see that there is still any missing values:

```{r , echo =TRUE}
check.na.values(train.data)
```

We fill the missing values in the testing set with the above-predicted values for each outlet as shown below:

```{r}
test_miss_size_i <- which(is.na(test.data$Outlet_Size))
for (i in test_miss_size_i) {
  out_size <- unique_outs[unique_outs$Outlet_Identifier == test.data[i,]$Outlet_Identifier,]$Outlet_Size
  test.data[i,]$Outlet_Size <- out_size
}
```

After this, we check if there is any missing values in the testing set:

```{r , echo =TRUE}
check.na.values(test.data)
```
Since there are no more missing values we proceed further with data cleaning.
After observing the Item_Fat_Content, we found that different labels represented same information. To fix that, remap the labels to only two logically significant labels, namely, low_fat and regular. 
```{r , echo =TRUE}
mapping_values <- c("low_fat", "low_fat", "low_fat", "regular", "regular")
names(mapping_values) <- c("LF", "low fat", "Low Fat", "reg", "Regular")

train.data <- train.data %>%
  mutate(Item_Fat_Content = mapping_values[Item_Fat_Content])

test.data <- test.data %>%
  mutate(Item_Fat_Content = mapping_values[Item_Fat_Content])
  
```

##Feature Engineering:  

1. Since the Outlet_Establishment_Year is in years, which is logically numeric, we transform it to the Years_Operating and then drop the column.
2. We created a feature named Item_Cat which represents the Category of the Item. It's created from the first two letters of Item_Identifier labels which represents the category of the products.
3. We then observed that some Non-consumables have either low_fat or regular, which doesn't really make sense. So we changed those labels accordingly.
4. We observed that a lot of Items with 0% visibility that have made sales. We fixed that by taking the aggregated visibility of the same item and setting the visibility to the obtained aggregated value
```{r , echo =TRUE}
train.data = train.data %>% 
  ####### 1.
  mutate(Years_Operating = 2013 - Outlet_Establishment_Year) %>% 
  ####### 2.
  mutate(Item_Cat = substr(Item_Identifier,1,2)) %>%   # rowwise() %>% 
  ####### 3.
  mutate(Item_Fat_Content = ifelse(Item_Cat=="NC","not_edible",Item_Fat_Content)) %>%  
  select(-c(Outlet_Establishment_Year)) 
###### 4.
train.data = train.data %>% 
  filter(Item_Visibility>0) %>% 
  group_by(Outlet_Identifier, Item_Type) %>% 
  summarise(Item_Visibility_Avg = mean(Item_Visibility)) %>% 
  merge(train.data, ., by=c("Outlet_Identifier", "Item_Type")) %>% 
  ungroup() %>% 
  mutate(Item_Visibility = ifelse(Item_Visibility==0,Item_Visibility_Avg,Item_Visibility) ) 
  
train.data = train.data %>% 
  mutate(Item_Visibility_Ratio = Item_Visibility/Item_Visibility_Avg ) %>% 
  select(-c(Item_Visibility_Avg)) 
  
```

Applying the same steps on the test data set, so that are models stay healthy for test set as well.

```{r , echo =TRUE}
test.data = test.data %>% 
  mutate(Years_Operating = 2013 - Outlet_Establishment_Year) %>% 
  mutate(Item_Cat = substr(Item_Identifier,1,2)) %>% 
  mutate(Item_Fat_Content = ifelse(Item_Cat=="NC","not_edible",Item_Fat_Content)) %>% 
  select(-c(Outlet_Establishment_Year))

test.data = test.data %>% 
  filter(Item_Visibility>0) %>% 
  group_by(Outlet_Identifier, Item_Type) %>% 
  summarise(Item_Visibility_Avg = mean(Item_Visibility)) %>% 
  merge(test.data, ., by=c("Outlet_Identifier", "Item_Type")) %>% 
  ungroup() %>% 
  mutate(Item_Visibility = ifelse(Item_Visibility==0,Item_Visibility_Avg,Item_Visibility) ) 
  
test.data = test.data %>% 
  mutate(Item_Visibility_Ratio = Item_Visibility/Item_Visibility_Avg ) %>% 
  select(-c(Item_Visibility_Avg)) 
  
```

```{r , echo =TRUE}
setdiff(names(train.data),names(test.data))
```

Finally, we split sample from our training set 1000 observations that we are going to keep aside(in the vault).

```{r}
set.seed(502)
split <- sample(1:nrow(train.data),1000)
aside.test.data <- train.data[split,]
train.data <- train.data[-split,]
train.data <- as.data.frame(unclass(train.data))
aside.test.data <- as.data.frame(unclass(aside.test.data))
```

### Data Exploration

First, we started by looking at the data to find any interesting relationships between our predictors.

One of the most obvious relashionships is looking at the distribution Retail Price of the Items (Item_MRP) in our training data. We observe that there are 4 major ranges of Item_MRP accross all the items. 

```{r mrp-sales}
ggplot(train.data)+
  geom_histogram(aes(x=Item_MRP),binwidth = 0.1)+
  ggtitle("Distribution of Item Prices")+
  xlab("List Price of Items")+
  ylab("Count")+
  geom_vline(xintercept = 69, color="red")+
  geom_vline(xintercept = 136, color="red")+
  geom_vline(xintercept = 203, color="red") 


```
We also try to plot the sales against the type of outlet colored based on the outlet size. We get some interesting observations such as sales for a given Outlet Size appear to be similar across Outlet Type. For example, Supermarket Type 1 has all three Outlet Sizes which all have about the same Sales. 
```{r outlet-sales}

ggplot(train.data[1:nrow(train.data),], aes(x = Outlet_Type, y = Item_Outlet_Sales, fill = Outlet_Size)) +
  geom_boxplot() +
  theme_gray() +
  xlab("Outlet type") + 
  ylab("Sales") + 
ggtitle("Sales vs Outlet type")


```
Next are the sales of each of the 10 outlets. The intuition behind the plot was to observe which outlets perform well and which do not. Through this plot we see that the two outlets that have extremely low sales are the Grocery Stores. 

```{r sales-out_id}
library(ggplot2)

ggplot(train.data[1:nrow(train.data),], aes(Outlet_Identifier, Item_Outlet_Sales)) +
  geom_boxplot(aes(color=Outlet_Type)) +
  theme_gray() +
  xlab("Outlet identifier") + 
  ylab("Sales") + 
  ggtitle("Sales vs Outlet Identifier")
```

## Simple Linear Models

```{r linear all vars, echo=TRUE}
linear.fit <- lm(Item_Outlet_Sales ~ ., data = subset(train.data, select = -Item_Identifier))
summary(linear.fit)
```

From the linear model, we can see that the sales of a particular item depends mainly on the store where it is sold and what the MRP is. The rmse is 1124. $R^{2}$ = 0.56. So only 56% of the variance in the output is explained by this linear model

```{r linear mrp, echo=TRUE}
linear.mrp.fit <- lm(Item_Outlet_Sales ~ Item_MRP, data = subset(train.data, select = -Item_Identifier))
summary(linear.mrp.fit)
```

From the summary, we see that $R^{2}$ has dropped to 0.3, far less than the previous linear model with all the variables.

```{r linear mrp, echo=TRUE}
linear.size.type.fit <- lm(Item_Outlet_Sales ~ Outlet_Size * Outlet_Type, data = subset(train.data, select = -Item_Identifier))
summary(linear.size.type.fit)
```
From the summary we see that the interactions are `NA` due to collinearity. That means the interation variables are some linear combination of the other variables and to solve the normal equation. We will park it for now, and then manually one hot encode and add infinitesimal noise to the dummy variables for the linear model to keep them.


### Principal Component Regression

In this section, we fit a Principal Component Regression model. First, we start by find the number of principal components needed to maximize the variance explanation and minimizing the MSE while keeping a reasonable number of components. In our case, the number of Principal Components chosen could be `27` (corresponds to the knee in the curve) as shown on the graphs below: 

```{r pcr 1, warning=FALSE}
library(pls)

pcr.data = subset(train.data, select = -Item_Identifier)

pcr.fit = pcr(Item_Outlet_Sales ~ ., data = pcr.data,center=T, scale = TRUE, validation = "CV", segments= 10)

# variance explanation
plot(cumsum(pcr.fit$Xvar/sum(pcr.fit$Xvar)), xlab = 'Number of Principal Components', ylab='Cumulative Proportion of Variance Explained')
abline(v=27, col="blue")

# validation plot
validationplot(pcr.fit,val.type='MSEP')
abline(v=27, col="blue")
```

Once the `ncomp` value chosen, we proceed to a 10-fold cross validation process where we estimate the RMSE for a Principal Component Regression model fitted with the first 27 PCs.

```{r pcr 2}
k = 10

ncomp = 27

set.seed(502)
folds = sample(rep(1:k, length = nrow(pcr.data)), replace = T)

cv.errors = rep(0, k)

for (i in 1:k) {
  pcr.fit <- pcr(Item_Outlet_Sales ~ ., data = pcr.data[folds != i,], scale = TRUE)
  pred = predict( pcr.fit, pcr.data[folds == i,], ncomp=ncomp)
  error = mean((pcr.data[folds == i,]$Item_Outlet_Sales - pred)^2)
  cv.errors[i] = error
}
sqrt(mean(cv.errors))

```

We can observe that the estimated value of the RMSE for this model is around `1126.671`.

### PCA

In this section, we are trying to reduce the number of our predictors by finding a normalized linear combination of the original predictors in a data set (41 predictors after hot enconding). In order to do this, we perform a Principal Component Analysis which is a generalization of the above-mentionned PCR where we extract the PCs to give us the possiblity to use them with any other model.

First, we fit our model and plot the proportion of variance explained vs. the number of first principal components chosen.

```{r pca 1}
# train.data.cat <- Filter(Negate(is.numeric),train.data)
# train.data.num <- Filter(is.numeric,train.data)
# train.data.dum <- dummy_cols(subset(train.data.cat, select = -Item_Identifier))[,-c(1:7)]
# train.data.pca <- data.frame(train.data.num,train.data.dum)

train.data.num <- model.matrix(Item_Outlet_Sales~.,subset(train.data, select = -Item_Identifier))
pca.out <- prcomp(train.data.num[,-1], scale=TRUE, center=TRUE)

myVar=pca.out$sdev^2
explained=myVar/sum(myVar)

plot(explained, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
plot(cumsum(explained), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')
abline(v=27, col="blue")
"PC27"
summary(pca.out)$importance[,27]
```

As expected, the number of principal components that corresponds to the knee in the curve is around `27`, which confirms the value previously chosen in the PCR section.

Now, we plot our data according to the 1st and 2nd Principal Components and then according to the 1st and 3rd Principal Components.

```{r pca 2}

Cols=function(vec){
  cols=rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}

plot(pca.out$x[,c(1,2)], col=Cols(log(train.data$Item_Outlet_Sales)), pch=19,xlab="PC1",ylab="PC2")
plot(pca.out$x[,c(1,3)], col=Cols(log(train.data$Item_Outlet_Sales)), pch=19,xlab="PC1",ylab="PC3")


# 3d plot of the first 3 significant components of PCA, gradient color of log of sales
color.gradient <- function(x, colors=c("green","yellow","red"), colsteps=100) {
  return(colorRampPalette(colors) (colsteps) [ findInterval(x, seq(min(x),max(x), length.out=colsteps)) ])
}

#library(rgl)
#plot3d(pca.out$x[,1], pca.out$x[,2], pca.out$x[,3], col = color.gradient(log(train.data$Item_Outlet_Sales + 1)))

```

We can observe that, unexpectedly, the representation of the data according to the first and second PCs presents well-spreaded, however, less homogenious repartition of the data accroding to the target class `Item_Outlet_Sales`. On the other side, both of these plots represent a poor visualization of our data since they explain barely 22.8% of the variance for the first two components and even less for the 1st and 3rd combined.

Finally, we assign transform the (aside) testing set according to the same Principal Component Analysis tranformation that resulted above.

```{r}
train.pca <- pca.out$x
aside.test.pca <- predict(pca.out, newdata= model.matrix(Item_Outlet_Sales~.,
                                                         subset(aside.test.data, select= -Item_Identifier)
                                                         )[,-1])
```

## Feature selection & Dimensionality reduction

After looking at the data, we noticed that, since our dataset is a mixture of categorical (7) and numerical (7) features, we are going to need to hot encode our data in order to be able to apply a big number of resgression algorithms.

That said, the fact that some of the categorical features have up to 10 different values (10 level factors) would introduce a lot of new features after dummy encoding and we finished up with 41 features after one hot encoding (the first generated feature is always dropped).
Hence, a reduction of the number of features should processed.

In the next sections, we are going to try different feature selectio and dimentionality reduction techniques and discuss them.

### Forward Feature Selection

First, we start with subset selection and given the fact that we cannot perform a best subset feature selection on our data due to the large computation complexity, we try to approximate it with forward feature selection as shown below:

```{r ffs, message=FALSE, warning=FALSE}
library(leaps)
forwardSubset = regsubsets(Item_Outlet_Sales~.,subset(train.data, select = -Item_Identifier), method='forward')
plot(forwardSubset,scale='r2')
knitr::kable(data.frame(selected_features = names(coef(forwardSubset,9))[-1]), caption='Forward Selection')
```

We can see that the forward selection outputs a 9 feature subset (8 + intercept) as an estimation of the best subset features selection with `Item_Visibility`, 3 of the hot encoded `Outlet_Identifier` columns (corresponding to 3 outlets), 2 of the `Outlet_Location` and one `Outlet_Size`. We can interpret that according to FFS, these features are more important than the other, i.e. for `Outlet_Size`, knowing if the outlet is of medium size or not matters more than knowing what exactly is the size of the outlet (small or big).

### Backward Feature Selection

In this section, we try again to estimate the best subset of features following another method which is backward subset selection as shown below:

```{r bfs, message=FALSE, warning=FALSE}
backwardSubset = regsubsets(Item_Outlet_Sales~.,subset(train.data, select = -Item_Identifier), method='backward')
plot(backwardSubset,scale='r2')
knitr::kable(data.frame(selected_features = names(coef(backwardSubset,9))[-1]), caption='Bakcward Selection')
```

We can observe clearly through the output of BFS that this estimation gives a high importance to the `Outlet_Identifier` feature and we can see again the `Item_Cat` corresponding to the value food which means that knowing if our item is a food product or not would help the prediction of the `Item_Outlet_Sales`.

### Lasso Regression

In this section, we are going to try to reduce the number of feature using Lasso. However, the tricky parts resides in choosing the $\lambda$ value corresponding to the best penalty for our case, namely, a $\lambda$ value that reduces the variance to prevent overfitting without increasing the bias too much. (yet another variance-bias tradeoff situation)

Hence, we execute a Lasso regression with different values of Lambda, we predict the sales for our validation (aside.test.data) set and then we calculate the error for each $\lambda$.

The plot below shows the evolution of the error with respect to different values of $\lambda$. The red point show the value corresponding to the lowest MSE.

```{r lasso 1, message=FALSE, warning=FALSE}
library(glmnet)
XTrain = model.matrix(Item_Outlet_Sales~.,                                                         subset(train.data, select= -Item_Identifier))

YTrain = train.data$Item_Outlet_Sales

XTest = model.matrix(Item_Outlet_Sales~.,                                                         subset(aside.test.data, select= -Item_Identifier))

YTest = aside.test.data$Item_Outlet_Sales

myLambda = 10^(seq(3,-1,length=100))

myFit = glmnet(XTrain,YTrain,alpha=1,lambda=myLambda)

myCoef = coef(myFit)
myPredict = predict(myFit,newx=XTest)

errors = NULL
for (i in 1:100) {
  errors[i] = mean( (YTest - myPredict[,i])^2 )
}

plot(myLambda,errors,type='l', xlab='Lambda', ylab='MSE')+
  points(x=c(myLambda[which.min(errors)]), y = min(errors), pch=16, col='red')

coefs = myCoef[-1,which.min(errors)]

knitr::kable(data.frame(selected_features=names(which(coefs != 0)), coefficients= coefs[coefs != 0])[-1])
cat('Lasso Regression RMSE: ',sqrt(min(error)))

```

As we see in the table above, all the `Outlet_Type` and `Outlet_Identifier` (almost) are kept for creating the linear regression model with the lowest RMSE. On the other side, the numerical variables `Item_MRP`and `Item_Visibility` are also kept for this regression.

Below, we can see the plot of the coefficients corresponding to the features for a given $\lambda$. The green dots correspond to the $\lambda\ value (`r myLambda[which.min(errors)]`) with the lowest error (`r sqrt(min(error))`).

```{r lasso 2, message=FALSE, warning=FALSE}
plot(myCoef[2:20,90],col='red',pch=1, ylab='Coefficients', xlab='Lambda')
points(myCoef[2:20,which.min(errors)],col='green',pch=16)
points(myCoef[2:20,30],col='yellow',pch=4)
points(myCoef[2:20,1],col='blue',pch=3)
```

We can clearly see that for a high value of $\lambda$ all the coefficients are set to zero which means that the penalty is too large for finding any feature important enough to be kept.

### PC-KNN

```{r knn pca, echo=TRUE}
require(FNN)

k.nn = 3:10
k.cv = 5
total_comps = dim(train.pca)[2]

folds = sample(rep(1:k.cv, length = nrow(train.pca)), replace = F)

cv.results <- matrix(data = 0,nrow = length(k.nn), ncol = total_comps - 1)
rownames(cv.results) <- k.nn
colnames(cv.results) <- 2:total_comps

for (neighbours in k.nn){
  for (ncomp in 2:total_comps){
    cv.rmse = rep(0, k.cv)
    for (i in 1:k.cv){
      knn.pca.fit <- FNN::knn.reg(train = train.pca[folds!=i,1:ncomp],
                              test = train.pca[folds==i,1:ncomp],
                              y = train.data$Item_Outlet_Sales[folds!=i],
                              k = neighbours)
      rmse = sqrt(mean((train.data$Item_Outlet_Sales[folds==i] - knn.pca.fit$pred)^2))
      cv.rmse[i] = rmse
    }
    cv.results[neighbours - 2, ncomp - 1] <- mean(cv.rmse)
  }
}

library(reshape2)
cv.results <- as.data.frame(cv.results)
cv.results$neighbours <- rownames(cv.results)
cv.results.melt <- melt(cv.results, id.vars = "neighbours")
names(cv.results.melt) <- c("neighbours", "ncomps", "rmse")

cv.results.melt = as.data.frame(lapply(cv.results.melt, as.numeric))
rgl::plot3d(cv.results.melt, col = color.gradient(cv.results.melt$rmse))



cv.rmse = rep(0, k.cv)
for (i in 1:k.cv){
  knn.pca.fit <- FNN::knn.reg(train = train.pca[folds!=i, c(1,3)],
                          test = train.pca[folds==i, c(1,3)],
                          y = train.data$Item_Outlet_Sales[folds!=i],
                          k = neighbours)
  rmse = sqrt(mean((train.data$Item_Outlet_Sales[folds==i] - knn.pca.fit$pred)^2))
  cv.rmse[i] = rmse
}
xgb.rmse = mean(cv.rmse)
cat('XGBoost RMSE: ',sgb.rmse)
```

## Tree-Based Models

### Simple Trees

We implement regression trees on our training data and run a 10-fold cross validation. All predictors except for Item_Identifier for building each tree. Through this approach we obtain a training RMSE of 1276. We will keep this in mind when we run a random Forrest later on in the report. 

```{r, message=FALSE, warning=FALSE}
library(tree)
train.tree <- subset(train.data, select=-Item_Identifier)

folds <- cut(seq(1,nrow(train.tree )),breaks=10,labels=FALSE)
tree.mse = array(0,10)
ptree.mse = array(0,10)
trainTree = array(0,10)
ptree.res = array(0,10)

for(i in 1:10){
  testIndexes <- which(folds==i,arr.ind=TRUE)
  valData <- train.tree[testIndexes, ]
  trainData <- train.tree[-testIndexes, ]
  
  myTree = tree(Item_Outlet_Sales~.,trainData,minsize=50, mindev=.0001)
  preds <- predict(myTree, newdata= valData)
  
  tree.mse[i] = sqrt(mean((valData$Item_Outlet_Sales - preds)^2))
  
  predsT <- predict(myTree, newdata= trainData)
  trainTree[i] = sqrt(mean((trainData$Item_Outlet_Sales - predsT)^2))
  
  myPTree = prune.tree(myTree,best=8)
  preds <- predict(myPTree, newdata=valData)
  ptree.mse[i] = sqrt(mean((valData$Item_Outlet_Sales - preds)^2))
  
  myPTreeT = prune.tree(myTree,best=8)
  predss <- predict(myPTreeT, newdata=trainData)
  ptree.res[i] = sqrt(mean((trainData$Item_Outlet_Sales - predss)^2))
}
tree.rmse = mean(tree.mse)
ptree.rmse = mean(ptree.mse)

cat('Simple tree residual RMSE: ', mean(trainTree),'\n')
cat('Pruned tree residual RMSE: ', mean(ptree.res),'\n')
cat('Simple Tree RMSE: ',tree.rmse,'\n')
cat('Pruned Tree RMSE: ',ptree.rmse,'\n')
```

We can see above that pruning the tree actually reduces the RMSE on the validation set (which is an estimation of the real RMSE), however, the complete tree outperforms the pruned tree on the training set. This could simply be explained by the fact that the big tree overfits the data.

On the other side, pruning a tree doesn't only prevent overfitting but helps also having clear visualizations of the tree as shown below:

```{r prune}
plot(myPTree)
text(myPTree, pretty = 1)
```
## Ensembles

### Boosting

```{r boost-comp}
library(gbm)

trainData <- subset(train.data, select=-Item_Identifier)
boo.mse = array(0,10)

for (i in 1:16) {
      trees = i*250
      
    myBoost = gbm(formula=Item_Outlet_Sales~., data=trainData, shrinkage=0.01,interaction.depth=3, n.trees=trees, verbose = T)

    preds <- predict(myBoost, newdata= trainData, n.trees=trees)

    boo.mse[i] = sqrt(mean((trainData$Item_Outlet_Sales - preds)^2))
}
plot(1:16 * 250,boo.mse, xlab='Number of trees', ylab='RMSE')+
abline(v=3750, col='blue')
```

```{r boosting}

boo.data<-train.data[sample(nrow(train.data)),]

ntrees = 3750


folds <- cut(seq(1,nrow(boo.data)),breaks=10,labels=FALSE)

boo.mse = array(0,10)
for(i in 1:10){
    testIndexes <- which(folds==i,arr.ind=TRUE)
    valData <- boo.data[testIndexes, ]
    trainData <- boo.data[-testIndexes, ]
    
    myBoost = gbm(formula=Item_Outlet_Sales~., data=subset(trainData, select=-Item_Identifier), shrinkage=0.01, interaction.depth=2, n.trees=ntrees)

    preds <- predict(myBoost, newdata= valData, n.trees=ntrees)

    boo.mse[i] = sqrt(mean((valData$Item_Outlet_Sales - preds)^2))
}

boo.rmse <- mean(boo.mse)

cat('Boosting RMSE: ',boo.rmse)

```

### XGBoost

```{r xgboost}

library(xgboost)
library(Matrix)
train.data.matrix <- train.data %>%
  # select(-c("Outlet_Identifier", "Item_Identifier"))
  select(-c("Outlet_Identifier", "Item_Identifier"))

X.train <- sparse.model.matrix(Item_Outlet_Sales ~ ., data = train.data.matrix)[, -1]

y.train <- train.data$Item_Outlet_Sales

best_param <- list()
best_rmse <- Inf
best_rounds <- NULL

for (iter in 1:100) {
  set.seed(502)
  param <- list(objective = "reg:linear",
                eval_metric = "rmse",
                max_depth = sample(1:10, 1),
                eta = runif(1, .01, .3)
  )
  cv.nfold <-  5 # 5-fold cross-validation
  cv.nrounds <- sample(seq(100,1000, 100), 1)
  set.seed(502)
  xgb.cv.fit <- xgb.cv(data = X.train, label = y.train,
                 params = param, nfold = cv.nfold, nthread = 4,
                 nrounds = cv.nrounds,
                 verbose = F, early_stopping_rounds = 2, maximize = FALSE)

  min_rmse_index  <-  xgb.cv.fit$best_iteration
  min_rmse <-  xgb.cv.fit$evaluation_log[min_rmse_index]$test_rmse_mean

  if (min_rmse < best_rmse) {
    best_rmse <- min_rmse
    best_param <- param
    best_rounds <- cv.nrounds
  }
}
  
```

```{r, echo=TRUE}
cat("best_param:")
best_param
cat("best_rmse:")
best_rmse
cat("best_rounds:")
cat(best_rounds)

set.seed(502)
best.xgb.cv.fit <- xgboost(data = X.train, label = y.train,
                 params = best_param, nfold = cv.nfold, nthread = 4,
                 nrounds = best_rounds,
                 verbose = F, maximize = FALSE)

# We plot iteration vs train rmse to detect knee in the plot, so that we don't overfit the model
plot(best.xgb.cv.fit$evaluation_log[, c(1,2)])

knee_rounds = 15
set.seed(502)
xgb.fit <- xgboost(data = X.train, label = y.train,
                 params = best_param, nfold = cv.nfold, nthread = 4,
                 nrounds = knee_rounds,
                 verbose = F, maximize = FALSE)

# RMSE of the model
tail(xgb.cv.fit$evaluation_log, 1)

# Plot importances
xgb.plot.importance(xgb.importance(model = xgb.fit))
```

### Bagging (Random forest)
In this section we now implement a random Forrest on the Training Data and run a 10-fold cross validation. Looking at the error plots vs. number of trees we decided each Forrest having 50 trees is sufficient in order to get a approximation of our Training RMSE. Additionally, by default within a given Forrest each tree is built on a random sample of 2/3 with 1/3 of the predictors. Through this approach we obtain a training RMSE of about 1185 which is a little better than the Tree RMSE of 1276.  
```{r rforest, message=FALSE, warning=FALSE}
library(randomForest)
train.rf <- subset(train.data, select=-Item_Identifier)

folds <- cut(seq(1,nrow(train.rf )),breaks=10,labels=FALSE)
rf.mse = array(0,10)

for(i in 1:10){
  testIndexes <- which(folds==i,arr.ind=TRUE)
  valData <- train.rf[testIndexes, ]
  trainData <- train.rf[-testIndexes, ]
  
  myForest = randomForest(formula=Item_Outlet_Sales~., data=trainData, ntree=50)
  preds <- predict(myForest, newdata= valData)
  
  rf.mse[i] = sqrt(mean((valData$Item_Outlet_Sales - preds)^2))
}
rf.rmse = mean(rf.mse)
```

### Natural Splines

As we observed above in the `Item_Outlet_Sales` vs. `Item_MRP` plot that the `Item_MRP` presents three different seperation between the data. These separations could be interpreted as knots where the underlying function could have changed. On the other side, we know that `Item_MRP` is a predictor of high importance according to the previous experiments.
In order to investigate this further, we tried to fit our model with a natural spline with three knots corresponding to the above-mentionned values.

We noticed also during the experiments that a natural spline could be fitted (better than other predictors but still poor) to `Item_Visibility_Ratio`.

```{r ns 1}
library(splines)

#Sample the data
set.seed(502)
myMRP = train.data[sample(1:nrow(train.data),300,replace=FALSE),]$Item_MRP
set.seed(502)
myVisibility = train.data[sample(1:nrow(train.data),300,replace=FALSE),]$Item_Visibility_Ratio
set.seed(502)
mySales = train.data[sample(1:nrow(train.data),300,replace=FALSE),]$Item_Outlet_Sales

FitMRP = lm(mySales~ns(myMRP,knots=c(69,136,203)))
FitVisibility = lm(mySales~ns(myVisibility,knots=c(0.5)))

# Plot the spline for a sequence of values (shape)
mrplims=range(myMRP)
grid = seq(from=mrplims[1],to=mrplims[2])
PredMRP = predict(FitMRP,data=grid,se=TRUE)

PredVisibility = predict(FitVisibility,data=grid,se=TRUE)


plot(myVisibility,mySales, col='red') +
points(myVisibility,PredVisibility$fit+2*PredVisibility$se,col='green',pch=20) +
abline(v=c(1,2))

plot(myMRP,mySales, col='red') +
points(myMRP,PredMRP$fit-3*PredMRP$se,col='green',pch=20) +
abline(v=c(69,136,203))


```

We can observe that the model is poorly fitted for `Item_Visibility_Ratio`, however, it presents a slightly better fit for `Item_MRP`.

After multiple experiments, the best fitting spline to the sample of data that we picked corresponds to the predicted value `-3` times the standard error as shown above.
Now we fit our model on the whole training data and calculate the RMSE.

```{r ns 2}

ns.data<-train.data[sample(nrow(train.data)),]


folds <- cut(seq(1,nrow(ns.data)),breaks=10,labels=FALSE)

ns.mse = array(0,10)
for(i in 1:10){
    testIndexes <- which(folds==i,arr.ind=TRUE)
    valData <- ns.data[testIndexes, ]
    trainData <- ns.data[-testIndexes, ]
    
    FitMRP = lm(Item_Outlet_Sales~ns(Item_MRP,knots=c(69,136,203)), data = trainData)
    PredMRP = predict(FitMRP,newdata=valData,se=TRUE)
    ns.mse[i] = sqrt(mean((PredMRP$fit - valData$Item_Outlet_Sales)^2))
}

ns.rmse <- mean(ns.mse)

cat('Natural Spline RMSE(Item_MRP: ',ns.rmse)

```

We can see that the model above is not performing well since its RMSE is around 1397.588 which is higher than the previous models.

We can conclude that this is due to a poor fit of the underlying function since it's based on only one predictor.

# Conclusion


