---
title: "Project DS502"
author: "Mahdi Alouane and Rahul Pande"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      tidy=TRUE,
                      fig.align='center',
                      tidy.opts=list(width.cutoff=60))
# https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
```

```{r , echo =TRUE}
train.data <- read.csv('./bigmart_train.csv', stringsAsFactors = FALSE, na.strings = c(""))
test.data <- read.csv('./bigmart_test.csv', stringsAsFactors = FALSE, na.strings = c(""))
```

```{r , echo =TRUE}
check.na.values <- function(df){
  colSums(apply(df, 2, is.na))
}
check.na.values(train.data)
check.na.values(test.data)
```

We have missing values in `Item_Weight` and `Outlet_Size`.


Now, we check the frequencies of categorical variables.

```{r , echo =TRUE}
col_types = lapply(train.data, class)
char_cols = names(col_types[col_types == 'character'])
lapply(train.data[, setdiff(char_cols, c("Item_Identifier"))], table)
```


We aggregate on outlet level to impute outlet size
```{r , echo =TRUE}
setdiff(unique(train.data$Outlet_Identifier), unique(test.data$Outlet_Identifier))
setdiff(unique(test.data$Outlet_Identifier), unique(train.data$Outlet_Identifier))
```

We see that there are no new stores in the test data that are not already encountered in the training data.

```{r , echo =TRUE}
library(tidyverse)
item_weights <- train.data %>%
  group_by(Item_Identifier) %>%
  summarise(
    l_u_weights = length(unique(Item_Weight)),
    u_weights = paste(sort(unique(Item_Weight), na.last = TRUE), collapse = " | ")) %>%
  arrange(desc(u_weights))

head(item_weights, 10)
```

We see that in some places weights are NA whereas it is not NA in other rows for the same item. We can just use weights from other observations where weight is not NA (For the same item). For this purpose the whole train and test datasets have been used to impute the missing information. Hence, we define the following function for treating these missing values.

```{r}
fill_missing_values <- function(file) {
  train_file <- read.csv("./bigmart_train.csv")
  test_file <- read.csv("./bigmart_test.csv")
  all <- rbind(subset(train_file, select = -Item_Outlet_Sales), test_file)
  train_file_i <- which(is.na(file$Item_Weight))
  
  for (i in train_file_i) {
    id <- file[i,]$Item_Identifier
    weight <- all[ all$Item_Identifier == as.character(id) & !is.na(all$Item_Weight),]$Item_Weight[1]
    file[i,]$Item_Weight <- weight
  }
  
  return (file)
  
}
```

Then, we call it and check if the problem is resolved.

```{r , echo =TRUE}
train.data <- fill_missing_values(train.data)
test.data <- fill_missing_values(test.data)
check.na.values(train.data)
check.na.values(test.data)
```

Since, there is no more missing values for the feature `Item_Weight`, we treat in the following section the missing values for the feature `Outlet_Size`.

Given that `Outlet_Size` is an outlet specific attribute, we first begin by printing all the outlets available in our training set (10 outlets in total). Hence, we figure out that the 2410 missing values in training set belong to only 3 outlets and the size of these outlets is also missing in the testing set.

```{r}
unique_outs <- unique(train.data %>% select(starts_with("Outlet_")))
unique_outs[,c(1,3)]
```

In order to achieve this, we start by transforming the categorical attributes into dummy variables using `One Hot Encoding` as shown below:

``` {r}
library(DMwR)
library(fastDummies)

train_num <- dummy_cols(unique_outs, select_columns = c('Outlet_Establishment_Year','Outlet_Location_Type','Outlet_Size','Outlet_Type'))
train_num$Outlet_Size_NA <- NULL
na_indices <- which(is.na(train_num$Outlet_Size))
train_num[is.na(train_num$Outlet_Size),c('Outlet_Size_Small','Outlet_Size_Medium','Outlet_Size_High')] <- NA
names(train_num[,-c(1:5)])
```

Then, we predict the missing values for `Outlet_Size` using the K-Nearest Neighbors with K=5. The algorithm reaches out for the 5 closest neighbors (after scaling) for each observation where the attribute is missing and according to a vote assigns a score using a weighted average (`meth='weighAvg`). Therefore, we compute the maximum among the three possible values (Small, Medium and High) and assign it to the corresponding observation.

```{r}
train_num_imp <- knnImputation(train_num[,-c(1:5)], k = 5, scale=T)
for (i in na_indices) {
  out_sizes <- train_num_imp[i,c('Outlet_Size_Medium','Outlet_Size_High','Outlet_Size_Small')]
  max_out_size <- gsub('Outlet_Size_','',colnames(out_sizes)[apply(out_sizes,1,which.max)])
  train.data[train.data$Outlet_Identifier == train_num[i,]$Outlet_Identifier,]$Outlet_Size <- max_out_size
  unique_outs[unique_outs$Outlet_Identifier == train_num[i,]$Outlet_Identifier,]$Outlet_Size <- max_out_size
}
```

We can see here the missing values and their prediction according to 5NN.

```{r}
unique_outs[na_indices,]
train_num_imp[na_indices,c('Outlet_Size_Medium','Outlet_Size_High','Outlet_Size_Small'
                                )]
```

Finally, we check to see that there is still any missing values:

```{r , echo =TRUE}
check.na.values(train.data)
```

We fill the missing values in the testing set with the above-predicted values for each outlet as shown below:

```{r}
test_miss_size_i <- which(is.na(test.data$Outlet_Size))
for (i in test_miss_size_i) {
  out_size <- unique_outs[unique_outs$Outlet_Identifier == test.data[i,]$Outlet_Identifier,]$Outlet_Size
  test.data[i,]$Outlet_Size <- out_size
}
```

After this, we check if there is any missing values in the testing set:

```{r , echo =TRUE}
check.na.values(test.data)
```

```{r , echo =TRUE}
mapping_values <- c("low_fat", "low_fat", "low_fat", "regular", "regular")
names(mapping_values) <- c("LF", "low fat", "Low Fat", "reg", "Regular")

train.data <- train.data %>%
  mutate(Item_Fat_Content = mapping_values[Item_Fat_Content])
  
```

##Feature Engineering:  

###Train:
```{r , echo =TRUE}
train.data_v2 = train.data %>% 
  mutate(Years_Operating = 2013 - Outlet_Establishment_Year) %>% 
  mutate(Item_Cat = substr(Item_Identifier,1,2)) %>% 
  # rowwise() %>% 
  mutate(Item_Fat_Content = ifelse(Item_Cat=="NC","not_edible",Item_Fat_Content)) %>% 
  select(-c(Outlet_Establishment_Year)) 

train.data_v3 = train.data_v2 %>% 
  filter(Item_Visibility>0) %>% 
  group_by(Item_Identifier) %>% 
  summarise(Item_Visibility_Avg = mean(Item_Visibility)) %>% 
  merge(train.data_v2, ., by="Item_Identifier") %>% 
  ungroup() %>% 
  mutate(Item_Visibility = ifelse(Item_Visibility==0,Item_Visibility_Avg,Item_Visibility) ) 
  
train.data_v3 = train.data_v3 %>% 
  mutate(Item_Visibility_Ratio = Item_Visibility/Item_Visibility_Avg ) %>% 
  select(-c(Item_Visibility_Avg)) 
  
```

###Test:
```{r , echo =TRUE}
test.data_v2 = test.data %>% 
  mutate(Years_Operating = 2013 - Outlet_Establishment_Year) %>% 
  mutate(Item_Cat = substr(Item_Identifier,1,2)) %>% 
  mutate(Item_Fat_Content = ifelse(Item_Cat=="NC","not_edible",Item_Fat_Content)) %>% 
  select(-c(Outlet_Establishment_Year))

test.data_v3 = test.data_v2 %>% 
  filter(Item_Visibility>0) %>% 
  group_by(Item_Identifier) %>% 
  summarise(Item_Visibility_Avg = mean(Item_Visibility)) %>% 
  merge(test.data_v2, ., by="Item_Identifier") %>% 
  ungroup() %>% 
  mutate(Item_Visibility = ifelse(Item_Visibility==0,Item_Visibility_Avg,Item_Visibility) ) 
  
test.data_v3 = test.data_v3 %>% 
  mutate(Item_Visibility_Ratio = Item_Visibility/Item_Visibility_Avg ) %>% 
  select(-c(Item_Visibility_Avg)) 
  
```

```{r , echo =TRUE}
setdiff(names(train.data_v3),names(test.data_v3))
```

```{r}
library(ggplot2)
ggplot(train.data_v3[1:nrow(train.data_v3),], aes(Outlet_Identifier, Item_Outlet_Sales)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 70, vjust = 0.5, color = "black")) + 
  xlab("Outlet identifier") + 
  ylab("Sales") + 
  ggtitle("Sales vs Outlet identifier")


```

``` {r}
library(data.table) #required library for this method
holder.Year = data.table(train.data_v3)[,sum(Item_Outlet_Sales), by = Outlet_Establishment_Year]
barplot(setNames(holder.Year$V1,holder.Year$Outlet_Identifier),width = 2, names.arg = unique(train.data_v3$Outlet_Establishment_Year))

```

```{r}

ggplot(train.data_v3, aes(x=Item_MRP)) + 
  geom_density(color = "blue", adjust=1/5) +
ggtitle("Density of Item MRP")

```
```{r}

ggplot(train.data_v3[1:nrow(train.data_v3),], aes(x = Outlet_Type, y = Item_Outlet_Sales, fill = Outlet_Size)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 70, vjust = 0.5, color = "black")) + 
  xlab("Outlet type") + 
  ylab("Sales") + 
ggtitle("Sales vs Outlet type")


```
```{r}
ggplot(train.data_v3[1:nrow(train.data_v3),], aes(x = Item_Type, y = Item_Outlet_Sales, fill = Outlet_Type)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 70, vjust = 0.5, color = "black")) + 
  xlab("Item type") + 
  ylab("Sales") + 
ggtitle("Sales vs Item type")
```

```{r}
ggplot(train.data_v3, aes(Item_Type, Item_Visibility, fill = Outlet_Size)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 70, vjust = 0.5, color = "black")) + 
  xlab("Item Type") + 
  ylab("Item Visibility") + 
ggtitle("Item visibility vs Item Type")
```

```{r}
ggplot(train.data_v3, aes(Outlet_Identifier, Item_Visibility)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 70, vjust = 0.5, color = "black")) + 
  xlab("Outlet_Identifier") + 
  ylab("Item Visibility") + 
ggtitle("Item visibility vs Outlet identifier")
```

```{r}
ggplot() + 
geom_density(aes(x=Item_Visibility), colour="red", data=train.data_v3)
```
```{r}
ggplot(train.data_v3[1:nrow(train.data_v3),], aes(Item_Visibility, Item_Outlet_Sales)) +
  geom_point(size = 2.5, aes(colour = factor(Outlet_Type))) +
  theme(axis.text.x = element_text(angle = 70, vjust = 0.5, color = "black")) + 
  xlab("Item Visibility") + 
  ylab("Item Outlet Sales") +
  ggtitle("Item Sales vs Item Visibility")

```
