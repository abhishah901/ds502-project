---
title: "Project DS502"
author: "Mahdi Alouane and Rahul Pande"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      tidy=TRUE,
                      fig.align='center',
                      tidy.opts=list(width.cutoff=60))
# https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
```

```{r , echo =TRUE}
train.data <- read.csv('./bigmart_train.csv', stringsAsFactors = FALSE, na.strings = c(""))
test.data <- read.csv('./bigmart_test.csv', stringsAsFactors = FALSE, na.strings = c(""))
```

```{r , echo =TRUE}
check.na.values <- function(df){
  colSums(apply(df, 2, is.na))
}
check.na.values(train.data)
check.na.values(test.data)
```

We have missing values in `Item_Weight` and `Outlet_Size`.


Now, we check the frequencies of categorical variables.

```{r , echo =TRUE}
col_types = lapply(train.data, class)
char_cols = names(col_types[col_types == 'character'])
lapply(train.data[, setdiff(char_cols, c("Item_Identifier"))], table)
```


We aggregate on outlet level to impute outlet size
```{r , echo =TRUE}
setdiff(unique(train.data$Outlet_Identifier), unique(test.data$Outlet_Identifier))
setdiff(unique(test.data$Outlet_Identifier), unique(train.data$Outlet_Identifier))
```

We see that there are no new stores in the test data that are not already encountered in the training data.

```{r , echo =TRUE}
library(tidyverse)
item_weights <- train.data %>%
  group_by(Item_Identifier) %>%
  summarise(
    l_u_weights = length(unique(Item_Weight)),
    u_weights = paste(sort(unique(Item_Weight), na.last = TRUE), collapse = " | ")) %>%
  arrange(desc(u_weights))

head(item_weights, 10)
```

We see that in some places weights are NA whereas it is not NA in other rows for the same item. We can just use weights from other observations where weight is not NA (For the same item). For this purpose the whole train and test datasets have been used to impute the missing information. Hence, we define the following function for treating these missing values.

```{r}
fill_missing_values <- function(file) {
  train_file <- read.csv("./bigmart_train.csv")
  test_file <- read.csv("./bigmart_test.csv")
  all <- rbind(subset(train_file, select = -Item_Outlet_Sales), test_file)
  train_file_i <- which(is.na(file$Item_Weight))
  
  for (i in train_file_i) {
    id <- file[i,]$Item_Identifier
    weight <- all[ all$Item_Identifier == as.character(id) & !is.na(all$Item_Weight),]$Item_Weight[1]
    file[i,]$Item_Weight <- weight
  }
  
  return (file)
  
}
```

Then, we call it and check if the problem is resolved.

```{r , echo =TRUE}
train.data <- fill_missing_values(train.data)
test.data <- fill_missing_values(test.data)
check.na.values(train.data)
check.na.values(test.data)
```

Since, there is no more missing values for the feature `Item_Weight`, we treat in the following section the missing values for the feature `Outlet_Size`.

Given that `Outlet_Size` is an outlet specific attribute, we first begin by printing all the outlets available in our training set (10 outlets in total). Hence, we figure out that the 2410 missing values in training set belong to only 3 outlets and the size of these outlets is also missing in the testing set.

```{r}
unique_outs <- unique(train.data %>% select(starts_with("Outlet_")))
unique_outs[,c(1,3)]
```

In order to achieve this, we start by transforming the categorical attributes into dummy variables using `One Hot Encoding` as shown below:

``` {r}
library(DMwR)
library(fastDummies)

train_num <- dummy_cols(unique_outs, select_columns = c('Outlet_Establishment_Year','Outlet_Location_Type','Outlet_Size','Outlet_Type'))
train_num$Outlet_Size_NA <- NULL
na_indices <- which(is.na(train_num$Outlet_Size))
train_num[is.na(train_num$Outlet_Size),c('Outlet_Size_Small','Outlet_Size_Medium','Outlet_Size_High')] <- NA
names(train_num[,-c(1:5)])
```

Then, we predict the missing values for `Outlet_Size` using the K-Nearest Neighbors with K=5. The algorithm reaches out for the 5 closest neighbors (after scaling) for each observation where the attribute is missing and according to a vote assigns a score using a weighted average (`meth='weighAvg`). Therefore, we compute the maximum among the three possible values (Small, Medium and High) and assign it to the corresponding observation.

```{r}
train_num_imp <- knnImputation(train_num[,-c(1:5)], k = 5, scale=T)
for (i in na_indices) {
  out_sizes <- train_num_imp[i,c('Outlet_Size_Medium','Outlet_Size_High','Outlet_Size_Small')]
  max_out_size <- gsub('Outlet_Size_','',colnames(out_sizes)[apply(out_sizes,1,which.max)])
  train.data[train.data$Outlet_Identifier == train_num[i,]$Outlet_Identifier,]$Outlet_Size <- max_out_size
  unique_outs[unique_outs$Outlet_Identifier == train_num[i,]$Outlet_Identifier,]$Outlet_Size <- max_out_size
}
```

We can see here the missing values and their prediction according to 5NN.

```{r}
unique_outs[na_indices,]
train_num_imp[na_indices,c('Outlet_Size_Medium','Outlet_Size_High','Outlet_Size_Small'
                                )]
```

Finally, we check to see that there is still any missing values:

```{r , echo =TRUE}
check.na.values(train.data)
```

We fill the missing values in the testing set with the above-predicted values for each outlet as shown below:

```{r}
test_miss_size_i <- which(is.na(test.data$Outlet_Size))
for (i in test_miss_size_i) {
  out_size <- unique_outs[unique_outs$Outlet_Identifier == test.data[i,]$Outlet_Identifier,]$Outlet_Size
  test.data[i,]$Outlet_Size <- out_size
}
```

After this, we check if there is any missing values in the testing set:

```{r , echo =TRUE}
check.na.values(test.data)
```

```{r , echo =TRUE}
mapping_values <- c("low_fat", "low_fat", "low_fat", "regular", "regular")
names(mapping_values) <- c("LF", "low fat", "Low Fat", "reg", "Regular")

train.data <- train.data %>%
  mutate(Item_Fat_Content = mapping_values[Item_Fat_Content])
  
```

##Feature Engineering:  

###Train:
```{r , echo =TRUE}
train.data = train.data %>% 
  mutate(Years_Operating = 2013 - Outlet_Establishment_Year) %>% 
  mutate(Item_Cat = substr(Item_Identifier,1,2)) %>% 
  # rowwise() %>% 
  mutate(Item_Fat_Content = ifelse(Item_Cat=="NC","not_edible",Item_Fat_Content)) %>% 
  select(-c(Outlet_Establishment_Year)) 

train.data = train.data %>% 
  filter(Item_Visibility>0) %>% 
  group_by(Outlet_Identifier, Item_Type) %>% 
  summarise(Item_Visibility_Avg = mean(Item_Visibility)) %>% 
  merge(train.data, ., by=c("Outlet_Identifier", "Item_Type")) %>% 
  ungroup() %>% 
  mutate(Item_Visibility = ifelse(Item_Visibility==0,Item_Visibility_Avg,Item_Visibility) ) 
  
train.data = train.data %>% 
  mutate(Item_Visibility_Ratio = Item_Visibility/Item_Visibility_Avg ) %>% 
  select(-c(Item_Visibility_Avg)) 
  
```

###Test:
```{r , echo =TRUE}
test.data = test.data %>% 
  mutate(Years_Operating = 2013 - Outlet_Establishment_Year) %>% 
  mutate(Item_Cat = substr(Item_Identifier,1,2)) %>% 
  mutate(Item_Fat_Content = ifelse(Item_Cat=="NC","not_edible",Item_Fat_Content)) %>% 
  select(-c(Outlet_Establishment_Year))

test.data = test.data %>% 
  filter(Item_Visibility>0) %>% 
  group_by(Outlet_Identifier, Item_Type) %>% 
  summarise(Item_Visibility_Avg = mean(Item_Visibility)) %>% 
  merge(test.data, ., by=c("Outlet_Identifier", "Item_Type")) %>% 
  ungroup() %>% 
  mutate(Item_Visibility = ifelse(Item_Visibility==0,Item_Visibility_Avg,Item_Visibility) ) 
  
test.data = test.data %>% 
  mutate(Item_Visibility_Ratio = Item_Visibility/Item_Visibility_Avg ) %>% 
  select(-c(Item_Visibility_Avg)) 
  
```

```{r , echo =TRUE}
setdiff(names(train.data),names(test.data))
```

Finally, we split sample from our training set 1000 observations that we are going to keep aside.

```{r}
set.seed(502)
split <- sample(1:nrow(train.data),1000)
aside.test.data <- train.data[split,]
train.data <- train.data[-split,]
train.data <- as.data.frame(unclass(train.data))
aside.test.data <- as.data.frame(unclass(aside.test.data))
```

### Data Exploration

First, we started by looking at the data to find any interesting relationships between our predictors.

One of the most obvious relashionships is between 

```{r mrp-sales}

ggplot(train.data, aes(x=Item_MRP), fill='blue') + 
  geom_density(adjust=1/6) +
  geom_vline(xintercept = 69, color="red")+
  geom_vline(xintercept = 136, color="red")+
geom_vline(xintercept = 203, color="red") +
  xlab("Max. Retail Price")+
ggtitle("Spread of Item MRP")

```

```{r outlet-sales}

ggplot(train.data[1:nrow(train.data),], aes(x = Outlet_Type, y = Item_Outlet_Sales, fill = Outlet_Size)) +
  geom_boxplot() +
  theme_gray() +
  xlab("Outlet type") + 
  ylab("Sales") + 
ggtitle("Sales vs Outlet type")


```

```{r sales-out_id}
library(ggplot2)
ggplot(train.data[1:nrow(train.data),], aes(Outlet_Identifier, Item_Outlet_Sales)) +
  geom_boxplot() +
  theme_gray() +
  xlab("Outlet identifier") + 
  ylab("Sales") + 
  ggtitle("Sales vs Outlet identifier")


```

### Linear Model

### Principal Component Regression

In this section, we fit a Principal Component Regression model. First, we start by find the number of principal components needed to maximize the variance explanation and minimizing the MSE while keeping a reasonable number of components. In our case, the number of Principal Components chosen could be `27` (corresponds to the knee in the curve) as shown on the graphs below: 

```{r pcr 1, warning=FALSE}
library(pls)

pcr.data = subset(train.data, select = -Item_Identifier)

pcr.fit = pcr(Item_Outlet_Sales ~ ., data = pcr.data,center=T, scale = TRUE, validation = "CV", segments= 10)

# variance explanation
plot(cumsum(pcr.fit$Xvar/sum(pcr.fit$Xvar)), xlab = 'Number of Principal Components', ylab='Cumulative Proportion of Variance Explained')
abline(v=27, col="blue")

# validation plot
validationplot(pcr.fit,val.type='MSEP')
abline(v=27, col="blue")
```

Once the `ncomp` value chosen, we proceed to a 10-fold cross validation process where we estimate the RMSE for a Principal Component Regression model fitted with the first 27 PCs.

```{r pcr 2}
k = 10

ncomp = 27

set.seed(502)
folds = sample(rep(1:k, length = nrow(pcr.data)), replace = T)

cv.errors = rep(0, k)

for (i in 1:k) {
  pcr.fit <- pcr(Item_Outlet_Sales ~ ., data = pcr.data[folds != i,], scale = TRUE)
  pred = predict( pcr.fit, pcr.data[folds == i,], ncomp=ncomp)
  error = mean((pcr.data[folds == i,]$Item_Outlet_Sales - pred)^2)
  cv.errors[i] = error
}
sqrt(mean(cv.errors))

```

We can observe that the estimated value of the RMSE for this model is around `1126.671`.

### PCA

In this section, we are trying to reduce the number of our predictors by finding a normalized linear combination of the original predictors in a data set (41 predictors after hot enconding). In order to do this, we perform a Principal Component Analysis which is a generalization of the above-mentionned PCR where we extract the PCs to give us the possiblity to use them with any other model.

First, we fit our model and plot the proportion of variance explained vs. the number of first principal components chosen.

```{r pca 1}
# train.data.cat <- Filter(Negate(is.numeric),train.data)
# train.data.num <- Filter(is.numeric,train.data)
# train.data.dum <- dummy_cols(subset(train.data.cat, select = -Item_Identifier))[,-c(1:7)]
# train.data.pca <- data.frame(train.data.num,train.data.dum)

train.data.num <- model.matrix(Item_Outlet_Sales~.,subset(train.data, select = -Item_Identifier))
pca.out <- prcomp(train.data.num[,-1], scale=TRUE, center=TRUE)

myVar=pca.out$sdev^2
explained=myVar/sum(myVar)

plot(explained, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
plot(cumsum(explained), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')
abline(v=27, col="blue")
"PC27"
summary(pca.out)$importance[,27]
```

As expected, the number of principal components that corresponds to the knee in the curve is around `27`, which confirms the value previously chosen in the PCR section.

Now, we plot our data according to the 1st and 2nd Principal Components and then according to the 1st and 3rd Principal Components.

```{r pca 2}

Cols=function(vec){
  cols=rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}

plot(pca.out$x[,c(1,2)], col=Cols(log(train.data$Item_Outlet_Sales)), pch=19,xlab="PC1",ylab="PC2")
plot(pca.out$x[,c(1,3)], col=Cols(log(train.data$Item_Outlet_Sales)), pch=19,xlab="PC1",ylab="PC3")


# 3d plot of the first 3 significant components of PCA, gradient color of log of sales
color.gradient <- function(x, colors=c("red","yellow","green"), colsteps=100) {
  return(colorRampPalette(colors) (colsteps) [ findInterval(x, seq(min(x),max(x), length.out=colsteps)) ])
}

#library(rgl)
#plot3d(pca.out$x[,1], pca.out$x[,2], pca.out$x[,3], col = color.gradient(log(train.data$Item_Outlet_Sales + 1)))

```

We can observe that, unexpectedly, the representation of the data according to the first and second PCs presents well-spreaded, however, less homogenious repartition of the data accroding to the target class `Item_Outlet_Sales`. On the other side, both of these plots represent a poor visualization of our data since they explain barely 22.8% of the variance for the first two components and even less for the 1st and 3rd combined.

Finally, we assign transform the (aside) testing set according to the same Principal Component Analysis tranformation that resulted above.

```{r}
train.pca <- pca.out$x
aside.test.pca <- predict(pca.out, newdata= model.matrix(Item_Outlet_Sales~.,
                                                         subset(aside.test.data, select= -Item_Identifier)
                                                         )[,-1])
```

### Forward Feature Selection
```{r message=FALSE}
library(leaps)
forwardSubset = regsubsets(Item_Outlet_Sales~.,subset(train.data, select = -Item_Identifier), method='backward')
summary(forwardSubset)
names(coef(forwardSubset,9))
plot(forwardSubset,scale='r2')
```

### Trees

```{r}
library(tree)
myTree = tree(Item_Outlet_Sales~.,subset(train.data, select=-Item_Identifier),minsize=50, mindev=.0001)
plot(myTree)
text(myTree,pretty=20)

preds <- predict(myTree,train.data)
sqrt(mean((train.data$Item_Outlet_Sales - preds)^2))

myTree = prune.tree(myTree,best=10)
plot(myTree)
text(myTree,pretty=1)
```

### Random Forest

```{r, echo=TRUE}

library(xgboost)
library(Matrix)
train.data.matrix <- train.data %>%
  # select(-c("Outlet_Identifier", "Item_Identifier"))
  select(-c("Outlet_Identifier", "Item_Identifier"))

X.train <- sparse.model.matrix(Item_Outlet_Sales ~ ., data = train.data.matrix)[, -1]

y.train <- train.data$Item_Outlet_Sales

best_param <- list()
best_rmse <- Inf
best_rounds <- NULL

for (iter in 1:100) {
  set.seed(502)
  param <- list(objective = "reg:linear",
                eval_metric = "rmse",
                max_depth = sample(1:10, 1),
                eta = runif(1, .01, .3)
  )
  cv.nfold <-  5 # 5-fold cross-validation
  cv.nrounds <- sample(seq(100,1000, 100), 1)
  set.seed(502)
  xgb.cv.fit <- xgb.cv(data = X.train, label = y.train,
                 params = param, nfold = cv.nfold, nthread = 4,
                 nrounds = cv.nrounds,
                 verbose = F, early_stopping_rounds = 2, maximize = FALSE)

  min_rmse_index  <-  xgb.cv.fit$best_iteration
  min_rmse <-  xgb.cv.fit$evaluation_log[min_rmse_index]$test_rmse_mean

  if (min_rmse < best_rmse) {
    best_rmse <- min_rmse
    best_param <- param
    best_rounds <- cv.nrounds
  }
}
  
```

```{r, echo=TRUE}
cat("best_param:")
best_param
cat("best_rmse:")
best_rmse
cat("best_rounds:")
cat(best_rounds)

set.seed(502)
best.xgb.cv.fit <- xgboost(data = X.train, label = y.train,
                 params = best_param, nfold = cv.nfold, nthread = 4,
                 nrounds = best_rounds,
                 verbose = F, maximize = FALSE)

# We plot iteration vs train rmse to detect knee in the plot, so that we don't overfit the model
plot(best.xgb.cv.fit$evaluation_log[, c(1,2)])

knee_rounds = 15
set.seed(502)
xgb.fit <- xgboost(data = X.train, label = y.train,
                 params = best_param, nfold = cv.nfold, nthread = 4,
                 nrounds = knee_rounds,
                 verbose = F, maximize = FALSE)

# RMSE of the model
tail(xgb.cv.fit$evaluation_log, 1)

# Plot importances
xgb.plot.importance(xgb.importance(model = xgb.fit))
```


```{r}
cv.forest <- rfcv(as.dataframe(unclass(train.data)),train.data$Item_Outlet_Sales,cv.fold=5, step=0.66)
```

### Natural Splines

As we observed above in the `Item_Outlet_Sales` vs. `Item_MRP` plot that the `Item_MRP` presents three different seperation between the data. These separations could be interpreted as knots where the underlying function could have changed. On the other side, we know that `Item_MRP` is a predictor of high importance according to the previous experiments.
In order to investigate this further, we tried to fit our model with a natural spline with three knots corresponding to the above-mentionned values.

We noticed also during the experiments that a natural spline could be fitted (better than other predictors but still poor) to `Item_Visibility_Ratio`.

```{r ns 1}
library(splines)

#Sample the data
set.seed(502)
myMRP = train.data[sample(1:nrow(train.data),300,replace=FALSE),]$Item_MRP
set.seed(502)
myVisibility = train.data[sample(1:nrow(train.data),300,replace=FALSE),]$Item_Visibility_Ratio
set.seed(502)
mySales = train.data[sample(1:nrow(train.data),300,replace=FALSE),]$Item_Outlet_Sales

FitMRP = lm(mySales~ns(myMRP,knots=c(69,136,203)))
FitVisibility = lm(mySales~ns(myVisibility,knots=c(0.5)))

# Plot the spline for a sequence of values (shape)
mrplims=range(myMRP)
grid = seq(from=mrplims[1],to=mrplims[2])
PredMRP = predict(FitMRP,data=grid,se=TRUE)

PredVisibility = predict(FitVisibility,data=grid,se=TRUE)


plot(myVisibility,mySales, col='red') +
points(myVisibility,PredVisibility$fit+2*PredVisibility$se,col='green',pch=20) +
abline(v=c(1,2))

plot(myMRP,mySales, col='red') +
points(myMRP,PredMRP$fit-3*PredMRP$se,col='green',pch=20) +
abline(v=c(69,136,203))


```

We can observe that the model is poorly fitted for `Item_Visibility_Ratio`, however, it presents a slightly better fit for `Item_MRP`.

After multiple experiments, the best fitting spline to the sample of data that we picked corresponds to the predicted value `-3` times the standard error as shown above.
Now we fit our model on the whole training data and calculate the RMSE.

```{r ns 2}

train.data<-train.data[sample(nrow(train.data)),]


folds <- cut(seq(1,nrow(train.data)),breaks=10,labels=FALSE)

ns.rmse = array(0,10)
for(i in 1:10){
    
    testIndexes <- which(folds==i,arr.ind=TRUE)
    valData <- train.data[testIndexes, ]
    trainData <- train.data[-testIndexes, ]
    
    FitMRP = lm(trainData$Item_Outlet_Sales~ns(trainData$Item_MRP,knots=c(69,136,203)))
    PredMRP = predict(FitMRP,newdata=trainData,se=TRUE)
    ns.rmse[i] = sqrt(mean((PredMRP$fit - trainData$Item_Outlet_Sales)^2))
}

avg.rmse <- mean(ns.rmse)

cat('Natural Spline RMSE(Item_MRP: ',avg.rmse)


```

We can see that the model above is not performing well since its RMSE is around 1397.588 which is higher than the previous models.

We can conclude that this is due to a poor fit of the underlying function since it's based on only one predictor.
